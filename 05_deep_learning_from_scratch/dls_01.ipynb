{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(array: ndarray, array_grad: ndarray):\n",
    "    assert array.shape == array_grad.shape, \\\n",
    "    f'''\n",
    "    Two ndarrays should have the same shape;\n",
    "    instead, first ndarray's shape is {array.shape}\n",
    "    and second ndarray's shape is {array_grad.shape}\n",
    "    '''\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <code>Operation</code> and <code>ParamOperation</code></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract class for any Operation\n",
    "class Operation(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_: ndarray):\n",
    "        '''\n",
    "        Stores input in the self._input instance variable \n",
    "        Calls the self._output() function\n",
    "        '''\n",
    "        self.input_ = input_\n",
    "        \n",
    "        self.output = self._output()\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "    def backward(self, output_grad: ndarray):\n",
    "        '''\n",
    "        Calls the self._input_grad() function.\n",
    "        Checks that the appropriate shapes match.\n",
    "        '''\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        \n",
    "        assert_same_shape(self.input_, self._input_grad)\n",
    "        \n",
    "        return self.input_grad\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _output(self): \n",
    "        '''\n",
    "        The output method must be defined for each Operation\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        The _input_grad method must be defined for each Operation\n",
    "        '''\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another abstract class for \"parameter\" operations\n",
    "class ParamOperation(Operation):\n",
    "    \n",
    "    def __init__(self, param: ndarray):\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "        \n",
    "    \n",
    "    def backward(self, output_grad: ndarray):\n",
    "        '''\n",
    "        Calls the self._input_grad and self._param_grad.\n",
    "        Checks appropriate shapes\n",
    "        '''\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        \n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def _param_grad(self, output_grad: ndarray):\n",
    "        '''\n",
    "        Every subclass of ParamOperation must implement _param_grad\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. weight multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "    '''\n",
    "    Weight multiplication operation for a neural network\n",
    "    '''\n",
    "    def __init__(self, W: ndarray):\n",
    "        '''Initialize Operation with self.param = W'''\n",
    "        super().__init__(W)\n",
    "        \n",
    "    \n",
    "    def _output(self) -> ndarray:\n",
    "        '''Compute the output'''\n",
    "        return np.dot(self.input_, self.param)\n",
    "    \n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray: \n",
    "        '''Compute the gradient'''\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1, 0)))\n",
    "    \n",
    "    \n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''Compute the parameter gradient'''\n",
    "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. bias add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    '''\n",
    "    Compute bias addition\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, B: ndarray):\n",
    "        \n",
    "        assert B.shape[0] == 1\n",
    "        \n",
    "        super().__init__(B)\n",
    "        \n",
    "    \n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Compute output\n",
    "        '''\n",
    "        return self.input_ + self.param\n",
    "    \n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray):\n",
    "        '''\n",
    "        Compute input gradient\n",
    "        '''\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "    \n",
    "    \n",
    "    def _param_grad(self, ouput_grad: ndarray):\n",
    "        '''Compute the param grad'''\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    '''\n",
    "    Sigmoid activaiton function\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self):\n",
    "        return 1.0 /(1.0 + np.exp(-1.0 * self.input_))\n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray): \n",
    "        sigmoid_backward = self.output * (1 - self.output) \n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "    '''\n",
    "    Identity activation function\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self):\n",
    "        return self.input_\n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray): \n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><code>Layer</code> and <code>Dense</code></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    '''\n",
    "    A layer of neurons in a neural network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, neurons: int):\n",
    "        '''\n",
    "        The number of neurons roughly corresponds to the breadth of the layer\n",
    "        '''\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params: list[ndarray] = []\n",
    "        self.param_grads: list[ndarray] = []\n",
    "        self.operations: list[Operation] = []\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def _setup_layer(self, num_in: int):\n",
    "        '''The _setup_layer function must be implemented for each layer'''\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_: ndarray):\n",
    "        '''\n",
    "        Passes input forward through a series of operations\n",
    "        '''\n",
    "        \n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "        \n",
    "        self.input_ = input_\n",
    "        \n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_)\n",
    "            \n",
    "        self.output = input_\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "    def backward(self, output_grad: ndarray):\n",
    "        '''\n",
    "        Passes output_grad backward through a series of operations\n",
    "        Checks appropriate shapes\n",
    "        '''\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "            \n",
    "        input_grad = output_grad\n",
    "        \n",
    "        self._param_grads()\n",
    "        \n",
    "        return input_grad\n",
    "    \n",
    "    def _param_grads(self):\n",
    "        '''\n",
    "        Extracts the _param_grads from a layer's operations\n",
    "        '''\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "        \n",
    "    \n",
    "    def _params(self):\n",
    "        '''Extract params'''\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    '''A fully connected layer which inherits from Layer'''\n",
    "    def __init__(self, neurons: int, activation: Operation = Sigmoid()):\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def _setup_layer(self, input_: ndarray):\n",
    "        '''Operations'''\n",
    "        \n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "            \n",
    "        self.params = []\n",
    "        \n",
    "        # weights\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "        \n",
    "        # bias \n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "        \n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation\n",
    "                          ]\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(ABC):\n",
    "    \"\"\"The loss of a neural network\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, prediction: ndarray, target: ndarray):\n",
    "        '''Computes the actual loss value'''\n",
    "        assert_same_shape(prediction, target)\n",
    "        \n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        \n",
    "        loss_value = self._output()\n",
    "        \n",
    "        return loss_value\n",
    "    \n",
    "    \n",
    "    def backward(self): \n",
    "        '''Computes gradient of the loss value with respect to the input to the loss function'''\n",
    "        self.input_grad = self._input_grad()\n",
    "        \n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "        \n",
    "        return self.input_grad\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def _output(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def _input_grad(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self):\n",
    "        loss = (\n",
    "            np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0]\n",
    "        )\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _input_grad(self):\n",
    "        ret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
