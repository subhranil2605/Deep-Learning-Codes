{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(array: ndarray, array_grad: ndarray):\n",
    "    assert array.shape == array_grad.shape, \\\n",
    "    f'''\n",
    "    Two ndarrays should have the same shape;\n",
    "    instead, first ndarray's shape is {array.shape}\n",
    "    and second ndarray's shape is {array_grad.shape}\n",
    "    '''\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <code>Operation</code> and <code>ParamOperation</code></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(ABC):\n",
    "    '''\n",
    "    Base class for \"operation\" in neural network\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Stores input in the instance variable self.input_\n",
    "        Calls the self._output() method\n",
    "        '''\n",
    "        self.input_: ndarray = input_\n",
    "            \n",
    "        self.output = self._output()\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calls the self._input_grad() function.\n",
    "        Checks that the appropriate shapes match\n",
    "        '''\n",
    "        assert_same_shape(self.ouput, output_grad)\n",
    "        \n",
    "        self.input_grad = self._input_grad(ouput_grad)\n",
    "        \n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        \n",
    "        return self.input_grad\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        The _output method must be defined for each Operation\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        The _input_grad method must be defined for each Operation\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "    '''\n",
    "    An operation with parameters.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, param: ndarray) -> ndarray:\n",
    "        '''\n",
    "        The ParamOperation method\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "        \n",
    "        \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calls the self._input_grad and self._param_grad\n",
    "        '''\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        \n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "        \n",
    "        return self.input_grad\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Every subclass of ParamOperation must implement _param_grad.\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "    '''\n",
    "    Weight multiplication operation for a neural network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, W: ndarray):\n",
    "        '''\n",
    "        Initialize Operation with self.param = W\n",
    "        '''\n",
    "        super().__init__(W)\n",
    "        \n",
    "    \n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Computes the output\n",
    "        '''\n",
    "        return np.dot(self.input_, self.param)\n",
    "    \n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Computes input gradient\n",
    "        '''\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1, 0)))\n",
    "    \n",
    "    \n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Computes parameter gradient\n",
    "        '''\n",
    "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    '''\n",
    "    Compute Bias Addition\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, B: ndarray):\n",
    "        '''\n",
    "        Initialize Operation with self.param = B\n",
    "        Check appropriate shape.\n",
    "        '''\n",
    "        assert B.shape[0] == 1\n",
    "        \n",
    "        super().__init__(B)\n",
    "        \n",
    "    \n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Computes the output\n",
    "        '''\n",
    "        return self.input_ + self.param\n",
    "    \n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Computes input gradient\n",
    "        '''\n",
    "        return np.ones_like(sel.input_) * output_grad\n",
    "    \n",
    "    \n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Computes parameter gradient\n",
    "        '''\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    '''\n",
    "    Identify activation function\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self): \n",
    "        return 1.0 / (1.0 + np.exp(-1.0 * self.input_))\n",
    "    \n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray):\n",
    "        '''\n",
    "        Computes input gradient\n",
    "        '''\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "    '''\n",
    "    Identify activation function\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "    def _output(self): \n",
    "        return self.input_\n",
    "    \n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray):\n",
    "        '''\n",
    "        Computes input gradient\n",
    "        '''\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    '''\n",
    "    A \"layer\" of neurons in a neural network.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, neurons: int):\n",
    "        '''\n",
    "        The number of neurons roughly corresponds to the breadth of the layer\n",
    "        '''\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params: list[ndarray] = list()\n",
    "        self.param_grads: list[ndarray] = list()\n",
    "        self.operations: list[Operation] = list()\n",
    "            \n",
    "            \n",
    "    @abstractmethod      \n",
    "    def _setup_layer(self, num_in: int) -> None:\n",
    "        '''\n",
    "        The _setup_layer function must be implemented for each layer\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Passes input forward through a series of operations.\n",
    "        '''\n",
    "        if se.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "            \n",
    "        self.input_ = input_\n",
    "        \n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_)\n",
    "            \n",
    "        self.output = input_\n",
    "            \n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Passes output_grad backward through a series of operations.\n",
    "        Checks appropriate shapes.\n",
    "        '''\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "            \n",
    "        input_grad = output_grad\n",
    "        \n",
    "        self._param_grads()\n",
    "        \n",
    "        return input_grad\n",
    "    \n",
    "    \n",
    "    def _param_grads(self) -> ndarray:\n",
    "        '''\n",
    "        Extracts the _param_grads from a layer's operations.\n",
    "        '''\n",
    "        \n",
    "        self.param_grads = [\n",
    "            operation.param_grad \n",
    "            for operation in self.operations \n",
    "            if issubclass(operation.__class__, ParamOperation)\n",
    "        ]\n",
    "                \n",
    "    \n",
    "    def _params(self) -> ndarray:\n",
    "        '''\n",
    "        Extract the _params from a layer's operations.\n",
    "        '''     \n",
    "        self.params = [\n",
    "            operation.param \n",
    "            for operation in self.operations \n",
    "            if issubclass(operation.__class__, ParamOperation)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    '''\n",
    "    A fully connected layer that inherits from \"Layer\".\n",
    "    '''\n",
    "    def __init__(self, neurons: int, activation: Operation = Sigmoid()) -> None:\n",
    "        '''\n",
    "        Requires an activation function upon inirialization.\n",
    "        '''\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "        \n",
    "    \n",
    "    def _setup_layer(self, input_: ndarray) -> None:\n",
    "        '''\n",
    "        Defines the operations of a fully connected layer.\n",
    "        '''\n",
    "        if self.seed: \n",
    "            np.random.seed(self.seed)\n",
    "            \n",
    "        self.params = []\n",
    "        \n",
    "        # weights\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "        \n",
    "        # bias\n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "        \n",
    "        self.operations = [\n",
    "            WeightMultiply(self.params[0]),\n",
    "            BiasAdd(self.params[1]),\n",
    "            self.activation\n",
    "        ]\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(ABC):\n",
    "    '''\n",
    "    The \"Loss\" of a neural network\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        '''\n",
    "        Computes the actual loss value\n",
    "        '''\n",
    "        assert_same_shape(prediction, target)\n",
    "        \n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        \n",
    "        loss_value = self._output()\n",
    "        \n",
    "        return loss_value\n",
    "    \n",
    "    def backward(self) -> ndarray:\n",
    "        '''\n",
    "        Computes the gradient of the loss value wrt the input to the loss function\n",
    "        '''\n",
    "        self.input_grad = self._input_grad()\n",
    "        \n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "        \n",
    "        return self.input_grad\n",
    "        \n",
    "        \n",
    "    \n",
    "    @abstractmethod\n",
    "    def _output(self) -> float:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    \n",
    "    def _output(self) -> float:\n",
    "        loss = np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0]\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _input_grad(self) -> ndarray:\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(ABC):\n",
    "    def __init__(self, layers: list[Layer], loss: Loss, seed: int = 1) -> None:\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                setattr(layer, \"seed\", self.seed)\n",
    "                \n",
    "    def forward(self, x_batch: ndarray) -> ndarray:\n",
    "        x_out = x_batch\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "            \n",
    "        return x_out\n",
    "    \n",
    "    def backward(self, loss_grad: ndarray) -> None:\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def train_batch(self, x_batch, y_batch): \n",
    "        predictions = self.forward(x_batch)\n",
    "        loss = self.loss.forward(predictions, y_batch)\n",
    "        self.backward(self.loss.backward())\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def param(self):\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "    \n",
    "    \n",
    "    def param_grads(self):\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(ABC):\n",
    "    def __init__(self, lr: float=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    @abstractmethod\n",
    "    def step(self): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr: float=0.01):\n",
    "        super().__init__(lr)\n",
    "        \n",
    "    def step(self):\n",
    "        for (param, param_grad) in zip(self.net.params(), self.net.param_grads()):\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class Trainer(ABC):\n",
    "    def __init__(self, net: NeuralNetwork, optim: Optimizer) -> None:\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = 1e9\n",
    "        setattr(self.optim, 'net', self.net)\n",
    "        \n",
    "    def generate_batches(self, X: ndarray, y: ndarray, size: int = 32) -> tuple[ndarray]:\n",
    "        assert X.shape[0] == y.shape[0], \\\n",
    "        f'''\n",
    "        features and target must have the same number of rows, instead\n",
    "        features has {X.shape[0]} and target has {y.shape[0]}\n",
    "        '''\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        \n",
    "        for ii in range(0, N, size):\n",
    "            X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "            \n",
    "            yield X_batch, y_batch\n",
    "            \n",
    "    def fit(self, X_train, y_train,\n",
    "            X_test, y_test,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32, \n",
    "            seed: int = 1, \n",
    "            restart: bool = True) -> None:\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "                \n",
    "            self.best_loss = 1e9\n",
    "            \n",
    "        for e in range(epochs):\n",
    "            \n",
    "            if (e+1) % eval_every == 0:\n",
    "                last_model = deepcopy(self.net)\n",
    "                \n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "            \n",
    "            batch_generator = self.generate_batches(X_train, y_train, batch_size)\n",
    "            \n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "\n",
    "                self.optim.step()\n",
    "                \n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                else:\n",
    "                    print(f\"\"\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, using the model from epoch {e+1-eval_every}\"\"\")\n",
    "                    self.net = last_model\n",
    "                    # ensure self.optim is still updating self.net\n",
    "                    setattr(self.optim, 'net', self.net)\n",
    "                    break\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true: ndarray, y_pred: ndarray):\n",
    "    '''\n",
    "    Compute mean absolute error for a neural network.\n",
    "    '''    \n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def rmse(y_true: ndarray, y_pred: ndarray):\n",
    "    '''\n",
    "    Compute root mean squared error for a neural network.\n",
    "    '''\n",
    "    return np.sqrt(np.mean(np.power(y_true - y_pred, 2)))\n",
    "\n",
    "def eval_regression_model(model: NeuralNetwork,\n",
    "                          X_test: ndarray,\n",
    "                          y_test: ndarray):\n",
    "    '''\n",
    "    Compute mae and rmse for a neural network.\n",
    "    '''\n",
    "    preds = model.forward(X_test)\n",
    "    preds = preds.reshape(-1, 1)\n",
    "    print(\"Mean absolute error: {:.2f}\".format(mae(preds, y_test)))\n",
    "    print()\n",
    "    print(\"Root mean squared error {:.2f}\".format(rmse(preds, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = NeuralNetwork(\n",
    "    layers=[Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "dl = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "\n",
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
